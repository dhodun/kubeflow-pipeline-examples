{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "GOOGLE_APPLICATION_CREDENTIALS=\"./user-gcp-sa.json\"\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = GOOGLE_APPLICATION_CREDENTIALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "client = kfp.Client(host='4709991555f6b0ae-dot-us-central1.notebooks.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoML Serverless ML (Taxi) on KFP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: set GCS location to experiment run\n",
    "#TODO: set num passengers out of categorical\n",
    "- TODO: run on unsampled datset\n",
    "- TODO: set split column to use fingerprint\n",
    "- TODO: Difference between table and dataset?\n",
    "TODO: try using a cleanup task with boolean in testing mode\n",
    "TODO: does generatinge new stats do anything?\n",
    "TODO: How to add name to condition\n",
    "TODO: Existing dataset as '' vs None?\n",
    "TODO: good documentation on how to trigger code after conditional loop if the conditional doesn't run, we working on more traditional code logic?\n",
    "TODO: in general, make clear what steps have inputs / outputs when or when they're not executed? get confusing to translate between argo and general python, None, etc\n",
    "TODO: AUtoML, wanted to make more modular, had to refactor becuase you need to supply target column path, not name, couldnt' do my shortcut where i skipped dataset creation, ideally you only know a priori dataset, and then column names, not full pahts?\n",
    "(might be more an issue with the op), also seems easies with new tables API?\n",
    "TODO: AutoML Tables, why is it in core when not supported?\n",
    "TODO: operand, couldn't compare with None so had to go to emtpy string\n",
    "AttributeError: 'NoneType' object has no attribute 'id' (happens when client not initiated or stale?)\n",
    "==None but not is None is supported\n",
    "\n",
    "Wanted to do some logic on dataset_create_id, but it has to basically be reference as input to another step (not condition) or it will only be an artifact, not a paramter?\n",
    "also, when i provide a paramater as teh input for another step, the variable name gets replaced in the UI as the paramater id, not the input name?\n",
    "\n",
    "\n",
    "Ultimatey understanding Argo helped a lot\n",
    "exit handler variations: https://argoproj.github.io/docs/argo/examples/README.html#exit-handlers\n",
    "\n",
    "https://github.com/sebinsua/k8s-argo-parallel-aggregate-workflow/blob/master/parallel-aggregate-workflow.yml\n",
    "https://github.com/argoproj/argo/issues/934\n",
    "\n",
    "how to programtically delete a model that is training?\n",
    "\n",
    "need for really basic python params, like LIMIT or adding to a query\n",
    "\n",
    "bigquery op needs to output the temporary table? so i can suck it into AUtoML tables?\n",
    "\n",
    "this is ok though?\n",
    "gs://{{inputs.parameters.gcs_bucket}}/{{inputs.parameters.gcs_temp_directory}}/bq_taxi_output-*.csv\n",
    "output_gcs_path='gs://{}/{}/bq_taxi_output-*.csv'.format(gcs_bucket, gcs_temp_directory)\n",
    "\n",
    "why do outputs not put out more in general? i.e. BQ one shows me all the files, as well as the fuzzy match?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "PROJECT_ID = 'dhodun1'\n",
    "COMPUTE_REGION = 'us-central1' # Currently us-central1 is only region\n",
    "BUCKET = 'dhodun1-central1'\n",
    "\n",
    "# Raw dataset, not cleaned\n",
    "QUERY = '''\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers\n",
    "FROM `nyc-tlc.yellow.trips`\n",
    "WHERE MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 100000) = 1\n",
    "'''\n",
    "\n",
    "QUERY_CLEAN_100k = '''\n",
    "CREATE OR REPLACE TABLE `dhodun1.kfp_tmp_dataset.taxi_automl_export`\n",
    "AS\n",
    "\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  pickup_datetime,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count*1.0 AS passengers,\n",
    "  ST_Distance(ST_GeogPoint(pickup_longitude, pickup_latitude), ST_GeogPoint(dropoff_longitude, dropoff_latitude)) AS euclidean\n",
    "  , CAST(EXTRACT(DAYOFWEEK FROM pickup_datetime) AS STRING) AS dayofweek\n",
    "  , CAST(EXTRACT(HOUR FROM pickup_datetime) AS STRING) AS hourofday\n",
    "FROM `nyc-tlc.yellow.trips`\n",
    "# The full dataset has 1+ Billion rows, let's take only 1 out of 1,000 (or 1 Million total)\n",
    "#WHERE MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))), 1000000) = 1\n",
    "WHERE\n",
    "  trip_distance > 0\n",
    "  AND fare_amount >= 2.5\n",
    "  AND pickup_longitude > -78\n",
    "  AND pickup_longitude < -70code\n",
    "  AND dropoff_longitude > -78\n",
    "  AND dropoff_longitude < -70\n",
    "  AND pickup_latitude > 37\n",
    "  AND pickup_latitude < 45\n",
    "  AND dropoff_latitude > 37\n",
    "  AND dropoff_latitude < 45\n",
    "  AND passenger_count > 0\n",
    "  LIMIT 100000000'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Op to change 'passengers' from categorical to numeric\n",
    "Should be fixed with *1.0 in BQ Query, but related to this BQ CSV export bug: https://b.corp.google.com/issues/143356550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nroot_mean_squared_error: 3.5239999294281006\nmean_absolute_error: 1.5740498304367065\nmean_absolute_percentage_error: 16.357690811157227\nr_squared: 0.851056694984436\nroot_mean_squared_log_error: nan\n\n"
    }
   ],
   "source": [
    "# Test on training model, can I delete it?\n",
    "from google.cloud import automl_v1beta1\n",
    "client = automl_v1beta1.TablesClient()\n",
    "\n",
    "# models = list(client.list_models(project=PROJECT_ID))\n",
    "# for model in models:\n",
    "#     print(model.name)\n",
    "\n",
    "evaluations = client.list_model_evaluations(model_name='projects/978546835329/locations/us-central1/models/TBL8170501692329033728')\n",
    "for evaluation in evaluations:\n",
    "    print(evaluation.regression_evaluation_metrics)\n",
    "\n",
    "\n",
    "#print(type(evals[0].regression_evaluation_metrics))\n",
    "\n",
    "#client.list_model_evaluations(model=)\n",
    "\n",
    "#client.get_model_evaluation(model_evaluation_name)\n",
    "\n",
    "# models = [s for s in client.list_models(project=PROJECT_ID)]\n",
    "# for model in models:\n",
    "#     # print(model.deployment_state)\n",
    "#     print(model.dataset_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp import components\n",
    "\n",
    "def get_automl_tables_regression_eval(\n",
    "    model_name: str,\n",
    "):\n",
    "    # Returns AutoML Regressions stats, if true sets as 'metric' as well\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.9.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)\n",
    "\n",
    "    from google.cloud import automl_v1beta1\n",
    "    client = automl_v1beta1.TablesClient()\n",
    "\n",
    "    evals = list(client.list_model_evaluations(model_name=model_name))\n",
    "    for eval in evals:\n",
    "        if eval.regression_evaluation_metrics is not None:\n",
    "            return(eval.regression_evaluation_metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nroot_mean_squared_error: 3.5239999294281006\nmean_absolute_error: 1.5740498304367065\nmean_absolute_percentage_error: 16.357690811157227\nr_squared: 0.851056694984436\nroot_mean_squared_log_error: nan\n\n"
    }
   ],
   "source": [
    "get_automl_tables_regression_eval(model_name='projects/978546835329/locations/us-central1/models/TBL8170501692329033728')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from kfp import dsl\n",
    "from kfp import components\n",
    "\n",
    "def set_automl_tables_column_type(\n",
    "    dataset_path: str,\n",
    "    column_display_name: str,\n",
    "    type_code: str,    \n",
    "):\n",
    "    # Updates AutuML Column with new column type, does trigger a new column statistics job? how do we check?\n",
    "    import sys\n",
    "    import subprocess\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'google-cloud-automl==0.9.0', '--quiet', '--no-warn-script-location'], env={'PIP_DISABLE_PIP_VERSION_CHECK': '1'}, check=True)\n",
    "    \n",
    "    from google.cloud import automl_v1beta1\n",
    "    client = automl_v1beta1.TablesClient()\n",
    "    \n",
    "    dataset = client.get_dataset(dataset_name=dataset_path)\n",
    "    \n",
    "    column_specs_response = client.list_column_specs(dataset=dataset)\n",
    "    column_specs = list(column_specs_response)\n",
    "    \n",
    "    for column in column_specs:\n",
    "        if column.display_name == column_display_name:\n",
    "            # This kicks off a new statistics job... how to check to see if it's done? Took ~ 1 minute this time\n",
    "            response = client.update_column_spec(column_spec_name=column.name, dataset=dataset, type_code=type_code)\n",
    "            print('Updated column: \"{}\" to type code {}. Generating new statistics now...'.format(column_display_name, type_code))\n",
    "\n",
    "set_automl_tables_column_type_op = components.func_to_container_op(set_automl_tables_column_type, base_image='python:3.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "fatal: destination path 'pipelines' already exists and is not an empty directory.\n"
    }
   ],
   "source": [
    "# AutoML Tables components\n",
    "! git clone https://github.com/kubeflow/pipelines.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import components\n",
    "\n",
    "component_store = components.ComponentStore(url_search_prefixes=['https://raw.githubusercontent.com/kubeflow/pipelines/0.1.36/components/'])\n",
    "\n",
    "automl_create_dataset_for_tables_op = component_store.load_component('gcp/automl/create_dataset_for_tables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import components\n",
    "\n",
    "component_store = components.ComponentStore(local_search_paths=['./pipelines/components'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_create_dataset_for_tables_op = component_store.load_component('gcp/automl/create_dataset_for_tables')\n",
    "automl_import_data_from_bigquery_op = component_store.load_component('gcp/automl/import_data_from_bigquery')\n",
    "automl_import_data_from_gcs_op = component_store.load_component('gcp/automl/import_data_from_gcs')\n",
    "automl_create_model_for_tables_op = component_store.load_component('gcp/automl/create_model_for_tables')\n",
    "prediction_service_batch_predict_op = component_store.load_component('gcp/automl/prediction_service_batch_predict')\n",
    "automl_split_dataset_table_column_names_op = component_store.load_component('gcp/automl/split_dataset_table_column_names')\n",
    "\n",
    "bigquery_query_op = component_store.load_component('gcp/bigquery/query')\n",
    "#automl_create_dataset_for_tables_op = comp.load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/b3179d86b239a08bf4884b50dbf3a9151da96d66/components/gcp/automl/create_dataset_for_tables/component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automl_create_dataset_for_tables_op(PROJECT_ID, COMPUTE_REGION, 'taxi_data',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Updated column: \"passengers\" to type code FLOAT64. Generating new statistics now...\n"
    }
   ],
   "source": [
    "set_automl_tables_column_type(dataset_path=dataset_path, column_display_name='passengers', type_code='FLOAT64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDict(dict):\n",
    "  def __getattr__(self, name):\n",
    "    if name in self:\n",
    "      return self[name]\n",
    "    else:\n",
    "      raise AttributeError(\"No such attribute: \" + name)\n",
    "\n",
    "# Define the pipeline\n",
    "\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "\n",
    "def serverless_automl_taxi(\n",
    "    gcp_project_id: str,\n",
    "    gcp_region: str,\n",
    "    query: str,\n",
    "    gcs_bucket: str,\n",
    "    gcs_temp_directory: str = 'ml-taxi/{}'.format(dsl.RUN_ID_PLACEHOLDER),\n",
    "    temp_dataset_id: str = '',\n",
    "    temp_table_name: str = '',\n",
    "    dataset_display_name: str = 'taxi_data',\n",
    "    dataset_location: str = 'US',\n",
    "    existing_dataset_path: str = '',\n",
    "    existing_dataset_id: str = '',\n",
    "    target_column_name: str = 'fare_amount',\n",
    "    model_display_name: str = 'taxi_data_model',\n",
    "    train_budget_milli_node_hours: 'Integer' = 1000,\n",
    "    bq_input_uri: str ='',\n",
    "):\n",
    "    \"\"\"Pipeline to train on Taxi data\"\"\"\n",
    "\n",
    "    new_dataset = True\n",
    "\n",
    "    output_gcs_path='gs://{}/{}/bq_taxi_output-*.csv'.format(gcs_bucket, gcs_temp_directory)\n",
    "\n",
    "    if new_dataset:\n",
    "\n",
    "        # Create dataset\n",
    "        create_dataset_task = automl_create_dataset_for_tables_op(\n",
    "            gcp_project_id=gcp_project_id,\n",
    "            gcp_region=gcp_region,\n",
    "            display_name=dataset_display_name,\n",
    "        )\n",
    "        '''\n",
    "        # Query to clean dataset and dump to GCS\n",
    "        bigquery_export_task = bigquery_query_op(\n",
    "            query=query,\n",
    "            project_id=gcp_project_id,\n",
    "            output_gcs_path=output_gcs_path,\n",
    "            dataset_location=dataset_location,\n",
    "        )\n",
    "\n",
    "        # Query to clean dataset and dump to GCS\n",
    "        bigquery_export_task = bigquery_query_op(\n",
    "            query=query,\n",
    "            project_id=gcp_project_id,\n",
    "            dataset_location=dataset_location,\n",
    "            dataset_id=temp_dataset_id,\n",
    "            table_id=temp_table_name,\n",
    "        )\n",
    "        \n",
    "        # Import data from GCS automl_import_data_from_gcs_op\n",
    "        import_data_task = automl_import_data_from_gcs_op(\n",
    "            dataset_path=create_dataset_task.outputs['dataset_path'],\n",
    "            input_uris=[output_gcs_path],\n",
    "        ).after(bigquery_export_task)\n",
    "        \n",
    "\n",
    "        # Import data from BQ automl_import_data_from_gcs_op\n",
    "        import_data_task = automl_import_data_from_bigquery_op(\n",
    "            dataset_path=create_dataset_task.outputs['dataset_path'],\n",
    "            input_uri=temp_table_name,\n",
    "        ).after(bigquery_export_task)'''\n",
    "\n",
    "        # Import data from BQ directly\n",
    "        import_data_task = automl_import_data_from_bigquery_op(\n",
    "            dataset_path=create_dataset_task.outputs['dataset_path'],\n",
    "            input_uri=bq_input_uri,\n",
    "        )\n",
    "\n",
    "        # Change 'passengers' column from categorical to numerical\n",
    "        # Should be fixed with *1.0 in BQ Query, but related to this BQ CSV export bug: https://b.corp.google.com/issues/143356550\n",
    "        set_passengers_numeric_task = set_automl_tables_column_type_op(\n",
    "            dataset_path=import_data_task.outputs['dataset_path'],\n",
    "            column_display_name='passengers',\n",
    "            type_code='FLOAT64',\n",
    "        )\n",
    "    else:\n",
    "        # Create the objects so that the rest of the pipeline will run\n",
    "        create_dataset_task = ObjectDict({\n",
    "            'outputs': {\n",
    "                'dataset_id': existing_dataset_id\n",
    "            }\n",
    "        })\n",
    "        import_data_task = ObjectDict({\n",
    "            'outputs': {\n",
    "                'dataset_path': existing_dataset_path\n",
    "            }\n",
    "        })\n",
    "\n",
    "\n",
    "    # Prepare column schemas\n",
    "    split_column_specs_task = automl_split_dataset_table_column_names_op(\n",
    "        dataset_path=import_data_task.outputs['dataset_path'],\n",
    "        table_index=0,\n",
    "        target_column_name=target_column_name,        \n",
    "    )\n",
    "    if new_dataset:\n",
    "        split_column_specs_task.after(set_passengers_numeric_task)\n",
    "\n",
    "    # Train a model\n",
    "    create_model_task = automl_create_model_for_tables_op(\n",
    "        gcp_project_id=gcp_project_id,\n",
    "        gcp_region=gcp_region,\n",
    "        display_name=model_display_name,\n",
    "        dataset_id=create_dataset_task.outputs['dataset_id'],\n",
    "        target_column_path=split_column_specs_task.outputs['target_column_path'],\n",
    "        # input_feature_column_paths=None, # All non-target columns will be used if None is passed\n",
    "        # input_feature_column_paths=split_column_specs_task.outputs['feature_column_paths'],\n",
    "        optimization_objective='MINIMIZE_RMSE',\n",
    "        train_budget_milli_node_hours=train_budget_milli_node_hours,\n",
    "    )\n",
    "    \n",
    "    from kfp.gcp import use_gcp_secret\n",
    "    kfp.dsl.get_pipeline_conf().add_op_transformer(use_gcp_secret('user-gcp-sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(serverless_automl_taxi, 'pipeline.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "Experiment link <a href=\"http://4709991555f6b0ae-dot-us-central1.notebooks.googleusercontent.com/#/experiments/details/403b44d2-8b03-499b-90b3-da7761b157fd\" target=\"_blank\" >here</a>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "Run link <a href=\"http://4709991555f6b0ae-dot-us-central1.notebooks.googleusercontent.com/#/runs/details/2f99b56d-1d4e-4f74-974b-f3075bef62dd\" target=\"_blank\" >here</a>",
      "text/plain": "<IPython.core.display.HTML object>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "RunPipelineResult(run_id=2f99b56d-1d4e-4f74-974b-f3075bef62dd)"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import automl\n",
    "\n",
    "#PROJECT_ID = 'dhodun1'\n",
    "#COMPUTE_REGION = 'us-central1' # Currently us-central1 is only region\n",
    "\n",
    "#location_path = automl.AutoMlClient().location_path(PROJECT_ID, COMPUTE_REGION)\n",
    "client = kfp.Client(host='4709991555f6b0ae-dot-us-central1.notebooks.googleusercontent.com')\n",
    "client.create_run_from_pipeline_func(\n",
    "    serverless_automl_taxi,\n",
    "    arguments=dict(\n",
    "        gcp_project_id=PROJECT_ID,\n",
    "        gcp_region=COMPUTE_REGION,\n",
    "        query=QUERY,\n",
    "        gcs_bucket=BUCKET,\n",
    "        dataset_display_name='taxi_data',\n",
    "        #existing_dataset_path='projects/978546835329/locations/us-central1/datasets/TBL9144959265608302592',\n",
    "        #existing_dataset_id='TBL9144959265608302592',\n",
    "        temp_dataset_id='kfp_tmp_dataset',\n",
    "        temp_table_name='taxi_export',\n",
    "        bq_input_uri='bq://dhodun1.kfp_tmp_dataset.taxi_automl_export',\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud container clusters list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud container clusters get-credentials kubeflow-marketplace-1 --zone us-central1-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'projects/978546835329/locations/us-central1/datasets/TBL5712864505831096320'\n",
    "client = automl.AutoMlClient()\n",
    "list_table_specs_response = client.list_table_specs(dataset_path)\n",
    "list_table_specs_response\n",
    "table_specs = [s for s in list_table_specs_response]\n",
    "print('table_specs=')\n",
    "print(table_specs)\n",
    "table_spec_name = table_specs[0].name\n",
    "\n",
    "list_column_specs_response = client.list_column_specs(table_spec_name)\n",
    "column_specs = [s for s in list_column_specs_response]\n",
    "#client.get_column_spec('passengers')\n",
    "for column in column_specs:\n",
    "    if column.display_name == 'passengers':\n",
    "        passenger_column = column.name\n",
    "\n",
    "print('column_specs=')\n",
    "#print(column_specs)\n",
    "\n",
    "column = client.get_column_spec(passenger_column)\n",
    "print(column)\n",
    "\n",
    "#client = automl_v1beta1.TablesClient()\n",
    "\n",
    "client.update_column_spec(column_spec_name=passenger_column, type_code='NUMERIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "column.name"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_specs[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.run_pipeline_func_on_cluster(\n",
    "    serverless_ml_taxi_pipeline,\n",
    "    arguments = dict(\n",
    "        gcp_project_id=PROJECT_ID,\n",
    "        gcp_region=COMPUTE_REGION,\n",
    "        display_name='taxi_data'\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}